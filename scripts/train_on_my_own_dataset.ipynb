{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 此book仅作为参考，无法直接运行"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/znfs/anaconda3/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv.runner import init_dist\n",
    "from mmcv.utils import Config, DictAction, get_git_hash\n",
    "\n",
    "from mmseg import __version__\n",
    "from mmseg.apis import set_random_seed, train_segmentor\n",
    "from mmseg.datasets import build_dataset\n",
    "from mmseg.models import build_segmentor\n",
    "from mmseg.utils import collect_env, get_root_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Train a segmentor')\n",
    "parser.add_argument('--config', \n",
    "                    type=str,\n",
    "                    help='train config file path')\n",
    "parser.add_argument(\n",
    "    '--work-dir', \n",
    "    type=str,\n",
    "    default='/home/znfs/2021/mmsegmentation/models',\n",
    "    help='the dir to save logs and models')\n",
    "parser.add_argument(\n",
    "    '--load-from', help='the checkpoint file to load weights from')\n",
    "parser.add_argument(\n",
    "    '--resume-from', help='the checkpoint file to resume from')\n",
    "parser.add_argument(\n",
    "    '--no-validate',\n",
    "    action='store_true',\n",
    "    help='whether not to evaluate the checkpoint during training')\n",
    "group_gpus = parser.add_mutually_exclusive_group()\n",
    "group_gpus.add_argument(\n",
    "    '--gpus',\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help='number of gpus to use '\n",
    "    '(only applicable to non-distributed training)')\n",
    "group_gpus.add_argument(\n",
    "    '--gpu-ids',\n",
    "    type=int,\n",
    "    nargs='+',\n",
    "    help='ids of gpus to use '\n",
    "    '(only applicable to non-distributed training)')\n",
    "parser.add_argument('--seed', type=int, default=None, help='random seed')\n",
    "parser.add_argument(\n",
    "    '--deterministic',\n",
    "    action='store_true',\n",
    "    help='whether to set deterministic options for CUDNN backend.')\n",
    "parser.add_argument(\n",
    "    '--options', nargs='+', action=DictAction, help='custom options')\n",
    "parser.add_argument(\n",
    "    '--launcher',\n",
    "    choices=['none', 'pytorch', 'slurm', 'mpi'],\n",
    "    default='none',\n",
    "    help='job launcher')\n",
    "parser.add_argument('--local_rank', type=int, default=0)\n",
    "args = parser.parse_args(args=['--config', '/configs/segformer/segformer_mit-b3_512x512_20k_Drivearea.py']) # 修改此处\n",
    "if 'LOCAL_RANK' not in os.environ:\n",
    "    os.environ['LOCAL_RANK'] = str(args.local_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 15:08:08,872 - mmseg - INFO - Environment info:\n",
      "------------------------------------------------------------\n",
      "sys.platform: linux\n",
      "Python: 3.8.3 (default, Jul  2 2020, 16:21:59) [GCC 7.3.0]\n",
      "CUDA available: True\n",
      "GPU 0: GeForce RTX 2080 Ti\n",
      "CUDA_HOME: /usr/local/cuda\n",
      "NVCC: Cuda compilation tools, release 10.2, V10.2.89\n",
      "GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "PyTorch: 1.7.1\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 10.2\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75\n",
      "  - CuDNN 7.6.5\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "TorchVision: 0.8.2\n",
      "OpenCV: 4.5.2\n",
      "MMCV: 1.3.9\n",
      "MMCV Compiler: GCC 7.5\n",
      "MMCV CUDA Compiler: 10.2\n",
      "MMSegmentation: 0.17.0+a7461d9\n",
      "------------------------------------------------------------\n",
      "\n",
      "2021-09-09 15:08:08,873 - mmseg - INFO - Distributed training: False\n",
      "2021-09-09 15:08:09,045 - mmseg - INFO - Config:\n",
      "norm_cfg = dict(type='BN', requires_grad=True)\n",
      "model = dict(\n",
      "    type='EncoderDecoder',\n",
      "    pretrained='pretrain/mit_b3.pth',\n",
      "    backbone=dict(\n",
      "        type='MixVisionTransformer',\n",
      "        in_channels=3,\n",
      "        embed_dims=64,\n",
      "        num_stages=4,\n",
      "        num_layers=[3, 4, 18, 3],\n",
      "        num_heads=[1, 2, 5, 8],\n",
      "        patch_sizes=[7, 3, 3, 3],\n",
      "        sr_ratios=[8, 4, 2, 1],\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        mlp_ratio=4,\n",
      "        qkv_bias=True,\n",
      "        drop_rate=0.0,\n",
      "        attn_drop_rate=0.0,\n",
      "        drop_path_rate=0.1),\n",
      "    decode_head=dict(\n",
      "        type='SegformerHead',\n",
      "        in_channels=[64, 128, 320, 512],\n",
      "        in_index=[0, 1, 2, 3],\n",
      "        channels=256,\n",
      "        dropout_ratio=0.1,\n",
      "        num_classes=3,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        align_corners=False,\n",
      "        loss_decode=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),\n",
      "    train_cfg=dict(),\n",
      "    test_cfg=dict(mode='whole'))\n",
      "dataset_type = 'DriveareaDataset'\n",
      "data_root = '/home/znfs/2021/DATASET/DriveArea'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "crop_size = (512, 512)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations'),\n",
      "    dict(type='Resize', img_scale=(1280, 720), ratio_range=(0.5, 2.0)),\n",
      "    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),\n",
      "    dict(type='RandomFlip', prob=0.5),\n",
      "    dict(type='PhotoMetricDistortion'),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1280, 720),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=4,\n",
      "    workers_per_gpu=0,\n",
      "    train=dict(\n",
      "        type='DriveareaDataset',\n",
      "        data_root='/home/znfs/2021/DATASET/DriveArea',\n",
      "        img_dir='img_dir/train',\n",
      "        ann_dir='ann_dir/train',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(type='Resize', img_scale=(1280, 720), ratio_range=(0.5, 2.0)),\n",
      "            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),\n",
      "            dict(type='RandomFlip', prob=0.5),\n",
      "            dict(type='PhotoMetricDistortion'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='DriveareaDataset',\n",
      "        data_root='/home/znfs/2021/DATASET/DriveArea',\n",
      "        img_dir='img_dir/val',\n",
      "        ann_dir='ann_dir/val',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1280, 720),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='DriveareaDataset',\n",
      "        data_root='/home/znfs/2021/DATASET/DriveArea',\n",
      "        img_dir='img_dir/val',\n",
      "        ann_dir='ann_dir/val',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1280, 720),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]))\n",
      "log_config = dict(\n",
      "    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "cudnn_benchmark = True\n",
      "optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
      "optimizer_config = dict()\n",
      "lr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=False)\n",
      "runner = dict(type='IterBasedRunner', max_iters=20000)\n",
      "checkpoint_config = dict(by_epoch=False, interval=2000)\n",
      "evaluation = dict(interval=2000, metric='mIoU', pre_eval=True)\n",
      "work_dir = '/home/znfs/2021/mmsegmentation/models'\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args.config=str('/home/znfs/2021/mmsegmentation/configs/segformer/segformer_mit-b3_512x512_20k_Road.py')\n",
    "cfg = Config.fromfile(args.config)\n",
    "if args.options is not None:\n",
    "    cfg.merge_from_dict(args.options)\n",
    "# set cudnn_benchmark\n",
    "if cfg.get('cudnn_benchmark', False):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# work_dir is determined in this priority: CLI > segment in file > filename\n",
    "if args.work_dir is not None:\n",
    "    # update configs according to CLI args if args.work_dir is not None\n",
    "    cfg.work_dir = args.work_dir\n",
    "elif cfg.get('work_dir', None) is None:\n",
    "    # use config filename as default work_dir if cfg.work_dir is None\n",
    "    cfg.work_dir = osp.join('./work_dirs',\n",
    "                            osp.splitext(osp.basename(args.config))[0])\n",
    "if args.load_from is not None:\n",
    "    cfg.load_from = args.load_from\n",
    "if args.resume_from is not None:\n",
    "    cfg.resume_from = args.resume_from\n",
    "if args.gpu_ids is not None:\n",
    "    cfg.gpu_ids = args.gpu_ids\n",
    "else:\n",
    "    cfg.gpu_ids = range(1) if args.gpus is None else range(args.gpus)\n",
    "\n",
    "# init distributed env first, since logger depends on the dist info.\n",
    "if args.launcher == 'none':\n",
    "    distributed = False\n",
    "else:\n",
    "    distributed = True\n",
    "    init_dist(args.launcher, **cfg.dist_params)\n",
    "    \n",
    "# create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "# dump config\n",
    "cfg.dump(osp.join(cfg.work_dir, osp.basename(args.config)))\n",
    "# init the logger before other steps\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n",
    "logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)\n",
    "\n",
    "# init the meta dict to record some important information such as\n",
    "# environment info and seed, which will be logged\n",
    "meta = dict()\n",
    "\n",
    "# log env info\n",
    "env_info_dict = collect_env()\n",
    "env_info = '\\n'.join([f'{k}: {v}' for k, v in env_info_dict.items()])\n",
    "dash_line = '-' * 60 + '\\n'\n",
    "logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n",
    "            dash_line)\n",
    "meta['env_info'] = env_info\n",
    "\n",
    "# log some basic info\n",
    "logger.info(f'Distributed training: {distributed}')\n",
    "logger.info(f'Config:\\n{cfg.pretty_text}')\n",
    "\n",
    "# set random seeds\n",
    "if args.seed is not None:\n",
    "    logger.info(f'Set random seed to {args.seed}, deterministic: '\n",
    "                f'{args.deterministic}')\n",
    "    set_random_seed(args.seed, deterministic=args.deterministic)\n",
    "cfg.seed = args.seed\n",
    "meta['seed'] = args.seed\n",
    "meta['exp_name'] = osp.basename(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/znfs/2021/mmsegmentation/mmseg/models/backbones/mit.py:315: UserWarning: DeprecationWarning: pretrained is a deprecated, please use \"init_cfg\" instead\n",
      "  warnings.warn('DeprecationWarning: pretrained is a deprecated, '\n",
      "2021-09-09 15:08:11,233 - mmseg - INFO - Use load_from_local loader\n",
      "2021-09-09 15:08:11,375 - mmseg - INFO - EncoderDecoder(\n",
      "  (backbone): MixVisionTransformer(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PatchEmbed(\n",
      "          (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "          (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "        (1): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
      "              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
      "              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))\n",
      "              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): PatchEmbed(\n",
      "          (projection): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "        (1): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): PatchEmbed(\n",
      "          (projection): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "        (1): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (3): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (4): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (5): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (6): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (7): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (8): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (9): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (10): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (11): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (12): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (13): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (14): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (15): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (16): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (17): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=320, out_features=320, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "              (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))\n",
      "              (norm): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): PatchEmbed(\n",
      "          (projection): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "        (1): ModuleList(\n",
      "          (0): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (1): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "          (2): TransformerEncoderLayer(\n",
      "            (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): EfficientMultiheadAttention(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "            (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "            (ffn): MixFFN(\n",
      "              (activate): GELU()\n",
      "              (layers): Sequential(\n",
      "                (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (1): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "                (2): GELU()\n",
      "                (3): Dropout(p=0.0, inplace=False)\n",
      "                (4): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                (5): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (dropout_layer): DropPath()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decode_head): SegformerHead(\n",
      "    input_transform=multiple_select, ignore_index=255, align_corners=False\n",
      "    (loss_decode): CrossEntropyLoss()\n",
      "    (conv_seg): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "    (convs): ModuleList(\n",
      "      (0): ConvModule(\n",
      "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activate): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): ConvModule(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activate): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ConvModule(\n",
      "        (conv): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activate): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): ConvModule(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activate): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (fusion_conv): ConvModule(\n",
      "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 15:08:11,416 - mmseg - INFO - Loaded 3872 images\n"
     ]
    }
   ],
   "source": [
    "model = build_segmentor(\n",
    "        cfg.model,\n",
    "        train_cfg=cfg.get('train_cfg'),\n",
    "        test_cfg=cfg.get('test_cfg'))\n",
    "model.init_weights()\n",
    "\n",
    "logger.info(model)\n",
    "\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "if len(cfg.workflow) == 2:\n",
    "    val_dataset = copy.deepcopy(cfg.data.val)\n",
    "    val_dataset.pipeline = cfg.data.train.pipeline\n",
    "    datasets.append(build_dataset(val_dataset))\n",
    "if cfg.checkpoint_config is not None:\n",
    "    # save mmseg version, config file content and class names in\n",
    "    # checkpoints as meta data\n",
    "    cfg.checkpoint_config.meta = dict(\n",
    "        mmseg_version=f'{__version__}+{get_git_hash()[:7]}',\n",
    "        config=cfg.pretty_text,\n",
    "        CLASSES=datasets[0].CLASSES,\n",
    "        PALETTE=datasets[0].PALETTE)\n",
    "# add an attribute for visualization convenience\n",
    "model.CLASSES = datasets[0].CLASSES\n",
    "# passing checkpoint meta for saving best checkpoint\n",
    "meta.update(cfg.checkpoint_config.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'DriveareaDataset', 'data_root': '/home/znfs/2021/DATASET/DriveArea', 'img_dir': 'img_dir/val', 'ann_dir': 'ann_dir/val', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (1280, 720), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}]}\n"
     ]
    }
   ],
   "source": [
    "print(cfg.data.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.datasets import build_dataloader\n",
    "\n",
    "\n",
    "\n",
    "data_loaders = [\n",
    "        build_dataloader(\n",
    "            ds,\n",
    "            cfg.data.samples_per_gpu,\n",
    "            cfg.data.workers_per_gpu,\n",
    "            # cfg.gpus will be ignored if distributed\n",
    "            len(cfg.gpu_ids),\n",
    "            dist=distributed,\n",
    "            seed=cfg.seed,\n",
    "            drop_last=True) for ds in datasets\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_metas': DataContainer({'filename': '/home/znfs/2021/DATASET/DriveArea/img_dir/train/firehydrant_0005.png', 'ori_filename': 'firehydrant_0005.png', 'ori_shape': (720, 1280, 3), 'img_shape': (512, 512, 3), 'pad_shape': (512, 512, 3), 'scale_factor': array([0.8578125 , 0.85833335, 0.8578125 , 0.85833335], dtype=float32), 'flip': True, 'flip_direction': 'horizontal', 'img_norm_cfg': {'mean': array([123.675, 116.28 , 103.53 ], dtype=float32), 'std': array([58.395, 57.12 , 57.375], dtype=float32), 'to_rgb': True}}), 'img': DataContainer(tensor([[[ 0.4851,  0.4851,  0.4851,  ..., -0.5253, -0.5253, -0.6109],\n",
      "         [ 0.4851,  0.4851,  0.4851,  ..., -0.5253, -0.5253, -0.6109],\n",
      "         [ 0.4851,  0.4851,  0.4851,  ..., -0.4739, -0.5253, -0.6109],\n",
      "         ...,\n",
      "         [-1.8953, -1.9124, -1.8610,  ..., -0.7993, -0.7308, -0.7308],\n",
      "         [-1.8782, -1.8439, -1.8610,  ..., -0.7308, -0.7137, -0.7479],\n",
      "         [-1.8610, -1.8439, -1.8439,  ..., -0.6452, -0.6623, -0.7137]],\n",
      "\n",
      "        [[ 0.6604,  0.6604,  0.6604,  ..., -0.4951, -0.4951, -0.5651],\n",
      "         [ 0.6604,  0.6604,  0.6604,  ..., -0.4776, -0.4951, -0.5826],\n",
      "         [ 0.6604,  0.6604,  0.6604,  ..., -0.4251, -0.4776, -0.5826],\n",
      "         ...,\n",
      "         [-1.6681, -1.6856, -1.6506,  ..., -0.6527, -0.5651, -0.5476],\n",
      "         [-1.6506, -1.6155, -1.6331,  ..., -0.5476, -0.5301, -0.5651],\n",
      "         [-1.6331, -1.6155, -1.6155,  ..., -0.4601, -0.4776, -0.5301]],\n",
      "\n",
      "        [[ 0.8448,  0.8448,  0.8448,  ..., -0.3753, -0.3753, -0.4450],\n",
      "         [ 0.8448,  0.8448,  0.8448,  ..., -0.3578, -0.3753, -0.4450],\n",
      "         [ 0.8448,  0.8448,  0.8448,  ..., -0.3055, -0.3404, -0.4275],\n",
      "         ...,\n",
      "         [-1.4210, -1.4384, -1.4036,  ..., -0.4450, -0.3404, -0.3404],\n",
      "         [-1.4036, -1.3687, -1.3861,  ..., -0.3404, -0.3230, -0.3578],\n",
      "         [-1.3861, -1.3513, -1.3513,  ..., -0.2707, -0.2707, -0.3404]]])), 'gt_semantic_seg': DataContainer(tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]]]))}\n"
     ]
    }
   ],
   "source": [
    "for ds in datasets:\n",
    "    print(ds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "{'img_metas': DataContainer({'filename': '/home/znfs/2021/DATASET/DriveArea/img_dir/train/firehydrant_0004.png', 'ori_filename': 'firehydrant_0004.png', 'ori_shape': (720, 1280, 3), 'img_shape': (512, 512, 3), 'pad_shape': (512, 512, 3), 'scale_factor': array([1.146875 , 1.1472223, 1.146875 , 1.1472223], dtype=float32), 'flip': False, 'flip_direction': 'horizontal', 'img_norm_cfg': {'mean': array([123.675, 116.28 , 103.53 ], dtype=float32), 'std': array([58.395, 57.12 , 57.375], dtype=float32), 'to_rgb': True}}), 'img': DataContainer(tensor([[[-1.4329, -1.3473, -1.3130,  ...,  2.2147,  2.2147,  2.2147],\n",
      "         [-1.3815, -1.3302, -1.2959,  ...,  2.2147,  2.2147,  2.2147],\n",
      "         [-1.3473, -1.3987, -1.3302,  ...,  2.1975,  2.1975,  2.1975],\n",
      "         ...,\n",
      "         [ 0.5193,  0.5193,  0.5536,  ..., -0.9363, -0.9363, -0.9534],\n",
      "         [ 0.6049,  0.6049,  0.6221,  ..., -0.9534, -0.9363, -0.9534],\n",
      "         [ 0.6392,  0.6563,  0.6563,  ..., -0.9192, -0.9534, -0.9705]],\n",
      "\n",
      "        [[-1.2129, -1.0728, -1.0203,  ...,  2.4286,  2.4286,  2.4286],\n",
      "         [-1.1604, -1.0728, -1.0378,  ...,  2.4286,  2.4286,  2.4286],\n",
      "         [-1.0903, -1.1429, -1.0728,  ...,  2.4286,  2.4286,  2.4286],\n",
      "         ...,\n",
      "         [ 0.6954,  0.6954,  0.7304,  ..., -0.8803, -0.8102, -0.8277],\n",
      "         [ 0.7829,  0.7479,  0.7654,  ..., -0.8277, -0.8102, -0.8277],\n",
      "         [ 0.8529,  0.8179,  0.8179,  ..., -0.7927, -0.8102, -0.8277]],\n",
      "\n",
      "        [[-1.1596, -1.0724, -1.0201,  ...,  2.6226,  2.6226,  2.6226],\n",
      "         [-1.1073, -1.0724, -1.0376,  ...,  2.6051,  2.6051,  2.6051],\n",
      "         [-1.0898, -1.1421, -1.1073,  ...,  2.5877,  2.5877,  2.5877],\n",
      "         ...,\n",
      "         [ 0.8274,  0.8274,  0.8797,  ..., -0.7587, -0.7064, -0.7064],\n",
      "         [ 0.9842,  0.9668,  0.9842,  ..., -0.7064, -0.7064, -0.7064],\n",
      "         [ 1.0539,  1.0539,  1.0539,  ..., -0.6890, -0.7064, -0.7064]]])), 'gt_semantic_seg': DataContainer(tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]]]))}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "temp_seg = inputs['gt_semantic_seg'].data\n",
    "temp_data = inputs['img'].data\n",
    "print(type(temp_data))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib auto\n",
    "mean=[123.675, 116.28, 103.53]\n",
    "std=[58.395, 57.12, 57.375]\n",
    "\n",
    "r = temp_data[0].data.numpy()*std[0]+mean[0]\n",
    "g = temp_data[1].data.numpy()*std[1]+mean[1]\n",
    "b = temp_data[2].data.numpy()*std[2]+mean[2]\n",
    "\n",
    "img=np.array([r,g,b]) #合成三通道图像， 此时数组的尺寸是3*224*224，我们需要的是224*224*3\n",
    "img=img.transpose(1,2,0).astype(np.uint8) \n",
    "print(img.shape)\n",
    "# img = Image.fromarray(temp_data[0].data.numpy().astype('uint8')).convert('RGB')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.ion()\n",
    "img = Image.fromarray(temp_seg[0].data.numpy().astype('uint8')).convert('RGB')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('drivearea', 'obstacle', 'other')\n",
      "[[1, 1, 1], [2, 2, 2], [0, 0, 0]]\n",
      "[<mmseg.datasets.drivearea.DriveareaDataset object at 0x7fef3703c610>]\n"
     ]
    }
   ],
   "source": [
    "print(datasets[0].CLASSES)\n",
    "print(datasets[0].PALETTE)\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mmseg.datasets.drivearea.DriveareaDataset object at 0x7f08ba85daf0>\n"
     ]
    }
   ],
   "source": [
    "for ds in datasets:\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}